# -*- coding: utf-8 -*-
"""solution_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVhe4Pk9OEy4m1Iyc1gFTvQ_EPH5bQx0
"""

!pip install pandas numpy faker

import pandas as pd
import numpy as np
from faker import Faker
import random
from datetime import datetime, timedelta
# Initialize Faker
fake = Faker()
random.seed(42)
np.random.seed(42)

# --- CONFIGURATION ---
NUM_STORES = 10
NUM_PRODUCTS = 50
NUM_CUSTOMERS = 1000  # Pool of registered customers
NUM_TRANSACTIONS = 50000
DAYS_OF_DATA = 180    # 6 months
START_DATE = datetime(2025, 7, 1)

print(f"Generating {NUM_TRANSACTIONS} transactions over {DAYS_OF_DATA} days...")

# ==========================================
# 1. REFERENCE TABLES (SNAPSHOT AS OF START_DATE)
# ==========================================

# --- 1. Stores ---
print("Generating Stores...")
stores_data = []
regions = ['North', 'South', 'East', 'West']
for i in range(1, NUM_STORES + 1):
    stores_data.append({
        'store_id': i,
        'store_name': f'Retail Hub {fake.city()}',
        'store_city': fake.city(),
        'store_region': random.choice(regions),
        'opening_date': fake.date_between(start_date='-5y', end_date='-1y')
    })
df_stores = pd.DataFrame(stores_data)

# --- 2. Products ---
print("Generating Products...")
products_data = []
categories = ['Electronics', 'Apparel', 'Home', 'Grocery']
for i in range(1, NUM_PRODUCTS + 1):
    products_data.append({
        'product_id': i,
        'product_name': fake.catch_phrase().title(),
        'product_category': random.choice(categories),
        'unit_price': round(random.uniform(5.0, 500.0), 2),
        'current_stock_level': random.randint(0, 500)
    })
df_products = pd.DataFrame(products_data)

# --- 3. Customer Details (OPENING SNAPSHOT) ---
print("Generating Customers (Opening Snapshot)...")
customers_data = []

# [FIX: SNAPSHOT LOGIC]
snapshot_cutoff = START_DATE - timedelta(days=1)

for i in range(1, NUM_CUSTOMERS + 1):
    last_purchase = fake.date_between(start_date='-365d', end_date=snapshot_cutoff)

    customers_data.append({
        'customer_id': i,
        'first_name': fake.first_name(),
        'Email': fake.email(),
        'loyalty_status': random.choice(['Bronze', 'Silver', 'Gold', 'Platinum']),
        'total_loyalty_points': random.randint(0, 5000),
        'last_purchase_date': last_purchase,
        'segment_id': 'Standard', # [FIX: NO NULLS] Set default to 'Standard' to aid ingestion
        'customer_phone': str(fake.msisdn()),
        'customer_since': fake.date_between(start_date='-5y', end_date=last_purchase)
    })
df_customers = pd.DataFrame(customers_data)

# --- 4. Promotion Details ---
print("Generating Promotions...")
promotions_data = [
    {'promotion_id': 0, 'promotion_name': 'No Promotion', 'start_date': START_DATE, 'end_date': START_DATE + timedelta(days=5000), 'discount_percentage': 0.00, 'applicable_category': 'None'},
    {'promotion_id': 101, 'promotion_name': 'Summer Sale', 'start_date': START_DATE, 'end_date': START_DATE + timedelta(days=30), 'discount_percentage': 0.15, 'applicable_category': 'Apparel'},
    {'promotion_id': 102, 'promotion_name': 'Tech Bonanza', 'start_date': START_DATE + timedelta(days=60), 'end_date': START_DATE + timedelta(days=90), 'discount_percentage': 0.10, 'applicable_category': 'Electronics'},
    {'promotion_id': 103, 'promotion_name': 'Home Makeover', 'start_date': START_DATE + timedelta(days=100), 'end_date': START_DATE + timedelta(days=120), 'discount_percentage': 0.20, 'applicable_category': 'Home'},
    {'promotion_id': 104, 'promotion_name': 'Grocery Saver', 'start_date': START_DATE, 'end_date': START_DATE + timedelta(days=180), 'discount_percentage': 0.05, 'applicable_category': 'Grocery'}
]
df_promotions = pd.DataFrame(promotions_data)

# --- 5. Loyalty Rules ---
print("Generating Loyalty Rules...")
loyalty_rules_data = [
    {'rule_id': 1, 'rule_name': 'Standard', 'points_per_unit_spend': 1.00, 'min_spend_threshold': 0.00, 'bonus_points': 0},
    {'rule_id': 2, 'rule_name': 'Big Spender', 'points_per_unit_spend': 1.50, 'min_spend_threshold': 100.00, 'bonus_points': 50},
    {'rule_id': 3, 'rule_name': 'Weekend', 'points_per_unit_spend': 2.00, 'min_spend_threshold': 50.00, 'bonus_points': 10}
]
df_loyalty_rules = pd.DataFrame(loyalty_rules_data)

# ==========================================
# 2. TRANSACTION DATA GENERATION
# ==========================================
print("Generating Transactions (this may take a moment)...")

unregistered_pool = [str(fake.msisdn()) for _ in range(2000)]
registered_phones = set(df_customers['customer_phone'].unique())
unregistered_pool = [p for p in unregistered_pool if p not in registered_phones]

sales_headers = []
line_items = []
line_item_counter = 1

store_ids = df_stores['store_id'].tolist()
cust_ids = df_customers['customer_id'].tolist()
prod_ids = df_products['product_id'].tolist()

for txn_i in range(1, NUM_TRANSACTIONS + 1):
    txn_id = f'TXN_{txn_i:07}'
    store_id = random.choice(store_ids)

    days_offset = random.randint(0, DAYS_OF_DATA)
    txn_date = START_DATE + timedelta(days=days_offset, hours=random.randint(9, 21), minutes=random.randint(0, 59))

    is_registered = random.random() < 0.80

    if is_registered:
        c_id = random.choice(cust_ids)
        c_phone = df_customers.loc[df_customers['customer_id'] == c_id, 'customer_phone'].values[0]
    else:
        c_id = None
        if random.random() < 0.30 and len(unregistered_pool) > 0:
            c_phone = random.choice(unregistered_pool)
        else:
            while True:
                new_phone = str(fake.msisdn())
                if new_phone not in registered_phones:
                    c_phone = new_phone
                    break

    num_items = random.randint(1, 6)
    txn_total = 0

    for _ in range(num_items):
        p_id = random.choice(prod_ids)
        product_row = df_products[df_products['product_id'] == p_id].iloc[0]
        base_price = product_row['unit_price']
        category = product_row['product_category']
        qty = random.randint(1, 4)

        promo_id = None
        discount = 0.0

        active_promos = df_promotions[
            (df_promotions['start_date'] <= txn_date) &
            (df_promotions['end_date'] >= txn_date) &
            (df_promotions['applicable_category'] == category)
        ]

        if not active_promos.empty:
            promo = active_promos.iloc[0]
            promo_id = promo['promotion_id']
            discount = float(promo['discount_percentage'])

        final_price = base_price * (1 - discount)
        line_total = round(final_price * qty, 2)
        txn_total += line_total

        line_items.append({
            'line_item_id': line_item_counter,
            'transaction_id': txn_id,
            'product_id': p_id,
            'promotion_id': promo_id,
            'Quantity': qty,
            'line_item_amount': line_total
        })
        line_item_counter += 1

    sales_headers.append({
        'transaction_id': txn_id,
        'customer_id': c_id,
        'Store_id': store_id,
        'transaction_date': txn_date,
        'total_amount': round(txn_total, 2),
        'Customer_phone': c_phone
    })

df_sales_header = pd.DataFrame(sales_headers)
df_sales_lines = pd.DataFrame(line_items)
df_sales_lines['promotion_id'] = df_sales_lines['promotion_id'].fillna(0).astype(int)

# ==========================================
# 3. DATA QUALITY & ANOMALY INJECTION
# ==========================================
print("Injecting Data Quality Anomalies...")

anomaly_log = []

# 1. Ghost Store
indices = np.random.choice(df_sales_header.index, size=50, replace=False)
df_sales_header.loc[indices, 'Store_id'] = 9999
for idx in indices:
    anomaly_log.append({'entity': 'Header', 'id': df_sales_header.loc[idx, 'transaction_id'], 'anomaly_type': 'Ghost Store'})

# 2. Future Dates
indices = np.random.choice(df_sales_header.index, size=20, replace=False)
df_sales_header.loc[indices, 'transaction_date'] = datetime(2030, 1, 1)
for idx in indices:
    anomaly_log.append({'entity': 'Header', 'id': df_sales_header.loc[idx, 'transaction_id'], 'anomaly_type': 'Future Date'})

# 3. Ghost Customer
max_cust_id = df_customers['customer_id'].max()
indices = np.random.choice(df_sales_header.index, size=30, replace=False)
mask = df_sales_header.loc[indices, 'customer_id'].notnull()
valid_indices = df_sales_header.loc[indices][mask].index
if len(valid_indices) > 0:
    df_sales_header.loc[valid_indices, 'customer_id'] = df_sales_header.loc[valid_indices, 'customer_id'].apply(lambda x: max_cust_id + random.randint(5000, 10000))
    for idx in valid_indices:
        anomaly_log.append({'entity': 'Header', 'id': df_sales_header.loc[idx, 'transaction_id'], 'anomaly_type': 'Ghost Customer'})

# 4. Header vs Line Item Mismatch
indices = np.random.choice(df_sales_header.index, size=40, replace=False)
df_sales_header.loc[indices, 'total_amount'] = df_sales_header.loc[indices, 'total_amount'] + np.random.uniform(1.0, 50.0)
for idx in indices:
    anomaly_log.append({'entity': 'Header', 'id': df_sales_header.loc[idx, 'transaction_id'], 'anomaly_type': 'Total Amount Mismatch'})

# 5. Negative Amounts
indices = np.random.choice(df_sales_lines.index, size=50, replace=False)
df_sales_lines.loc[indices, 'line_item_amount'] = df_sales_lines.loc[indices, 'line_item_amount'] * -1
for idx in indices:
    anomaly_log.append({'entity': 'LineItem', 'id': df_sales_lines.loc[idx, 'line_item_id'], 'anomaly_type': 'Negative Amount'})

# 6. Ghost Product
max_prod_id = df_products['product_id'].max()
indices = np.random.choice(df_sales_lines.index, size=30, replace=False)
df_sales_lines.loc[indices, 'product_id'] = max_prod_id + 999
for idx in indices:
    anomaly_log.append({'entity': 'LineItem', 'id': df_sales_lines.loc[idx, 'line_item_id'], 'anomaly_type': 'Ghost Product'})

df_anomalies = pd.DataFrame(anomaly_log)
print("Anomalies Injected.")

# ==========================================
# 4. OPTION 1: NEW CUSTOMER (Validation Set)
# ==========================================
print("Generating Validation Set for Option 1...")

registered_phones_final = df_customers['customer_phone'].unique()
unidentified_txns = df_sales_header[
    (df_sales_header['customer_id'].isnull()) &
    (~df_sales_header['Customer_phone'].isin(registered_phones_final))
].copy()

df_new_customers = unidentified_txns.drop_duplicates(subset=['Customer_phone']).copy()
df_new_customers['temp_customer_id'] = [f'TEMP_{x:05}' for x in range(len(df_new_customers))]
df_new_customers['acquisition_source'] = np.random.choice(['Guest WiFi', 'POS Prompt', 'Digital Receipt'], size=len(df_new_customers))
df_new_customers['registration_likelihood'] = np.random.uniform(0.1, 0.9, size=len(df_new_customers)).round(2)

df_new_customers_final = df_new_customers[[
    'temp_customer_id',
    'Customer_phone',
    'transaction_date',
    'total_amount',
    'acquisition_source',
    'registration_likelihood'
]].rename(columns={
    'Customer_phone': 'detected_phone',
    'transaction_date': 'first_seen_date',
    'total_amount': 'initial_basket_value'
})

# ==========================================
# 5. GENERATE "GROUND TRUTH" ANSWER KEY (WITH LOYALTY SOLVER)
# ==========================================
print("Generating Ground Truth Answer Key (Calculating Points & RFM)...")

df_full_sales = df_sales_header.merge(df_sales_lines, on='transaction_id', how='left')
df_full_sales['transaction_date'] = pd.to_datetime(df_full_sales['transaction_date'], errors='coerce')

bad_txn_ids = df_anomalies[df_anomalies['entity'] == 'Header']['id'].unique()
df_clean_sales = df_full_sales[~df_full_sales['transaction_id'].isin(bad_txn_ids)].copy()

# A. LOYALTY POINT CALCULATION (SOLVER)
def calculate_points(row):
    # Rule 1: Standard (1pt per $1)
    pts_std = row['total_amount'] * 1.0

    # Rule 2: Big Spender (1.5pt per $1 + 50 bonus if > 100)
    pts_big = 0
    if row['total_amount'] > 100:
        pts_big = (row['total_amount'] * 1.5) + 50

    # Rule 3: Weekend (2.0pt per $1 + 10 bonus if Sat/Sun)
    pts_wknd = 0
    if row['transaction_date'].weekday() >= 5: # 5=Sat, 6=Sun
        pts_wknd = (row['total_amount'] * 2.0) + 10

    return max(pts_std, pts_big, pts_wknd)

df_clean_headers = df_clean_sales[['transaction_id', 'customer_id', 'total_amount', 'transaction_date']].drop_duplicates()
df_clean_headers['earned_points'] = df_clean_headers.apply(calculate_points, axis=1)
earned_points_agg = df_clean_headers.groupby('customer_id')['earned_points'].sum().reset_index()

# B. RFM CALCULATION
expected_sales = df_clean_sales.groupby('customer_id')['line_item_amount'].sum().reset_index()
expected_sales.rename(columns={'line_item_amount': 'monetary_value'}, inplace=True)

max_date = df_full_sales['transaction_date'].max()
rfm_agg = df_clean_sales.groupby('customer_id').agg({
    'transaction_date': lambda x: (max_date - x.max()).days, # Recency
    'transaction_id': 'nunique', # Frequency
}).reset_index()
rfm_agg.rename(columns={'transaction_date': 'recency', 'transaction_id': 'frequency'}, inplace=True)

# C. MERGE EVERYTHING
df_ground_truth = df_customers.copy()
df_ground_truth = df_ground_truth.merge(expected_sales, on='customer_id', how='left').fillna(0)
df_ground_truth = df_ground_truth.merge(rfm_agg, on='customer_id', how='left')
df_ground_truth = df_ground_truth.merge(earned_points_agg, on='customer_id', how='left').fillna(0)

df_ground_truth['final_expected_points'] = df_ground_truth['total_loyalty_points'] + df_ground_truth['earned_points']
df_ground_truth['recency'] = df_ground_truth['recency'].fillna(999)
df_ground_truth['frequency'] = df_ground_truth['frequency'].fillna(0)

# D. SEGMENTATION
top_10_cutoff = df_ground_truth['monetary_value'].quantile(0.90)
df_ground_truth.loc[df_ground_truth['monetary_value'] >= top_10_cutoff, 'expected_segment'] = 'High-Spender'
df_ground_truth.loc[
    (df_ground_truth['recency'] > 30) &
    (df_ground_truth['total_loyalty_points'] > 0) &
    (df_ground_truth['expected_segment'].isnull()),
    'expected_segment'
] = 'At-Risk'
df_ground_truth['expected_segment'].fillna('Standard', inplace=True)

# ==========================================
# 6. DDL GENERATION
# ==========================================
def generate_ddl():
    print("\n" + "="*40)
    print("      GENERATED SQL DDL STATEMENTS")
    print("="*40)

    ddl = """
/* STAGING LAYER (NO CONSTRAINTS) */
CREATE TABLE staging_store_sales_header (
    transaction_id VARCHAR(50),
    customer_id VARCHAR(50),
    Store_id VARCHAR(50),
    transaction_date VARCHAR(50),
    total_amount DECIMAL(10, 2),
    Customer_phone VARCHAR(20)
);

CREATE TABLE rejected_records (
    record_id INT AUTO_INCREMENT PRIMARY KEY,
    source_file VARCHAR(50),
    original_data JSON,
    rejection_reason VARCHAR(255),
    rejection_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);

/* CORE LAYER (STRICT) */
CREATE TABLE stores (
    store_id INT PRIMARY KEY,
    store_name VARCHAR(100),
    store_city VARCHAR(50),
    store_region VARCHAR(50),
    opening_date DATE
);

CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    product_category VARCHAR(50),
    unit_price DECIMAL(10, 2),
    current_stock_level INT
);

CREATE TABLE customer_details (
    customer_id INT PRIMARY KEY,
    first_name VARCHAR(50),
    Email VARCHAR(100),
    loyalty_status VARCHAR(20),
    total_loyalty_points INT,
    last_purchase_date DATE,
    segment_id VARCHAR(50),
    customer_phone VARCHAR(20), -- [COMMENT: Using VARCHAR to preserve formatting vs LONG]
    customer_since DATE
);

CREATE TABLE promotion_details (
    promotion_id INT PRIMARY KEY,
    promotion_name VARCHAR(100),
    start_date DATE,
    end_date DATE,
    discount_percentage DECIMAL(5, 2),
    applicable_category VARCHAR(50)
);

CREATE TABLE loyalty_rules (
    rule_id INT PRIMARY KEY,
    rule_name VARCHAR(100),
    points_per_unit_spend DECIMAL(5, 2),
    min_spend_threshold DECIMAL(10, 2),
    bonus_points INT
);

CREATE TABLE store_sales_header (
    transaction_id VARCHAR(30) PRIMARY KEY,
    customer_id INT,
    Store_id INT,
    transaction_date DATETIME,
    total_amount DECIMAL(10, 2),
    Customer_phone VARCHAR(20),
    FOREIGN KEY (customer_id) REFERENCES customer_details(customer_id),
    FOREIGN KEY (Store_id) REFERENCES stores(store_id)
);

CREATE TABLE store_sales_line_items (
    line_item_id INT PRIMARY KEY,
    transaction_id VARCHAR(30),
    product_id INT,
    promotion_id INT,
    Quantity INT,
    line_item_amount DECIMAL(10, 2),
    FOREIGN KEY (transaction_id) REFERENCES store_sales_header(transaction_id),
    FOREIGN KEY (product_id) REFERENCES products(product_id)
);

CREATE TABLE new_customer (
    temp_customer_id VARCHAR(20) PRIMARY KEY,
    detected_phone VARCHAR(20),
    first_seen_date DATETIME,
    initial_basket_value DECIMAL(10, 2),
    acquisition_source VARCHAR(50),
    registration_likelihood DECIMAL(3, 2)
);

/* DDL COMMENTS */
-- COMMENT ON TABLE new_customer IS 'Track unregistered customers via POS phone capture.';
-- COMMENT ON COLUMN new_customer.acquisition_source IS 'Channel tracking for marketing attribution.';
-- COMMENT ON COLUMN new_customer.registration_likelihood IS 'Score to prioritize conversion campaigns.';
    """
    print(ddl)

# ==========================================
# 7. EXPORT TO CSV
# ==========================================
print("Exporting to CSV...")

df_stores['opening_date'] = df_stores['opening_date'].astype(str)
df_customers['last_purchase_date'] = df_customers['last_purchase_date'].astype(str)
df_customers['customer_since'] = df_customers['customer_since'].astype(str)
df_promotions['start_date'] = df_promotions['start_date'].astype(str)
df_promotions['end_date'] = df_promotions['end_date'].astype(str)
df_sales_header['transaction_date'] = df_sales_header['transaction_date'].astype(str)

df_customers['customer_phone'] = df_customers['customer_phone'].astype(str)
df_sales_header['Customer_phone'] = df_sales_header['Customer_phone'].astype(str)
df_new_customers_final['detected_phone'] = df_new_customers_final['detected_phone'].astype(str)

df_stores.to_csv('stores.csv', index=False)
df_products.to_csv('products.csv', index=False)
df_customers.to_csv('customer_details.csv', index=False)
df_promotions.to_csv('promotion_details.csv', index=False)
df_loyalty_rules.to_csv('loyalty_rules.csv', index=False)
df_sales_header.to_csv('store_sales_header.csv', index=False)
df_sales_lines.to_csv('store_sales_line_items.csv', index=False)

df_new_customers_final.to_csv('validation_ground_truth_new_customer.csv', index=False)
df_anomalies.to_csv('validation_anomalies.csv', index=False)
df_ground_truth.to_csv('validation_ground_truth_customers.csv', index=False)

generate_ddl()
print("\nSUCCESS! All files created, DDL (w/ Rejections) generated, Answer Keys exported.")

"""# PART 2: ETL Pipeline & Database Processing"""

import sqlite3
import pandas as pd
import json

# --- 1. SETUP & CLEANUP ---
conn = sqlite3.connect('retail_hackathon.db')
cursor = conn.cursor()
cursor.execute("PRAGMA foreign_keys = OFF;") # Turn off checks while dropping

# WIPE OLD DATA (Fixes 'UNIQUE constraint failed')
tables = [
    'store_sales_line_items', 'store_sales_header', 'customer_details',
    'products', 'stores', 'promotion_details', 'rejected_records', 'new_customer'
]
for t in tables:
    cursor.execute(f"DROP TABLE IF EXISTS {t}")

cursor.execute("PRAGMA foreign_keys = ON;") # Turn checks back on for safety
print("ðŸ§¹ Database wiped clean.")

# --- 2. DDL: RE-CREATE TABLES ---
ddl_statements = [
    """CREATE TABLE rejected_records (
        record_id INTEGER PRIMARY KEY AUTOINCREMENT,
        entity_name TEXT,
        transaction_id TEXT,
        rejection_reason TEXT,
        raw_data TEXT,
        rejection_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    );""",
    """CREATE TABLE stores (
        store_id INTEGER PRIMARY KEY,
        store_name TEXT,
        store_city TEXT,
        store_region TEXT,
        opening_date DATE
    );""",
    """CREATE TABLE products (
        product_id INTEGER PRIMARY KEY,
        product_name TEXT,
        product_category TEXT,
        unit_price DECIMAL(10, 2),
        current_stock_level INTEGER
    );""",
    """CREATE TABLE customer_details (
        customer_id INTEGER PRIMARY KEY,
        first_name TEXT,
        email TEXT,
        loyalty_status TEXT,
        total_loyalty_points INTEGER,
        last_purchase_date DATE,
        segment_id TEXT,
        customer_phone TEXT,
        customer_since DATE
    );""",
    """CREATE TABLE promotion_details (
        promotion_id INTEGER PRIMARY KEY,
        promotion_name TEXT,
        start_date DATE,
        end_date DATE,
        discount_percentage DECIMAL(5, 2),
        applicable_category TEXT
    );""",
    """CREATE TABLE store_sales_header (
        transaction_id TEXT PRIMARY KEY,
        customer_id INTEGER,
        store_id INTEGER,
        transaction_date DATETIME,
        total_amount DECIMAL(10, 2),
        customer_phone TEXT,
        FOREIGN KEY (customer_id) REFERENCES customer_details(customer_id),
        FOREIGN KEY (store_id) REFERENCES stores(store_id)
    );""",
    """CREATE TABLE store_sales_line_items (
        line_item_id INTEGER PRIMARY KEY,
        transaction_id TEXT,
        product_id INTEGER,
        promotion_id INTEGER,
        quantity INTEGER,
        line_item_amount DECIMAL(10, 2),
        FOREIGN KEY (transaction_id) REFERENCES store_sales_header(transaction_id),
        FOREIGN KEY (product_id) REFERENCES products(product_id)
    );""",
    """CREATE TABLE new_customer (
        temp_customer_id TEXT PRIMARY KEY,
        detected_phone TEXT,
        first_seen_date DATETIME,
        initial_basket_value DECIMAL(10, 2),
        acquisition_source TEXT,
        registration_likelihood DECIMAL(3, 2)
    );"""
]
for sql in ddl_statements:
    cursor.execute(sql)
conn.commit()
print("âœ… Tables re-created.")

# --- 3. LOAD REFERENCE DATA ---
# (if_exists='append' works now because tables are empty)
pd.read_csv('stores.csv').to_sql('stores', conn, if_exists='append', index=False)
pd.read_csv('products.csv').to_sql('products', conn, if_exists='append', index=False)
pd.read_csv('promotion_details.csv').to_sql('promotion_details', conn, if_exists='append', index=False)

df_cust = pd.read_csv('customer_details.csv')
df_cust['customer_phone'] = df_cust['customer_phone'].astype(str)
df_cust.to_sql('customer_details', conn, if_exists='append', index=False)
print("âœ… Reference Data Loaded.")

# --- 4. DEFINE VALIDATION LOGIC (WITH GHOST CUST FIX) ---
valid_customer_ids = set(df_cust['customer_id'].unique())

def validate_sales_header(row):
    errors = []
    # Rule 1: Future Dates
    if pd.to_datetime(row['transaction_date']) > pd.Timestamp.now() + pd.Timedelta(days=1):
        errors.append("Transaction date in future")
    # Rule 2: Ghost Store
    if row['Store_id'] == 9999:
        errors.append("Invalid Store ID")
    # Rule 3: Ghost Customer (The FIX)
    c_id = row['customer_id']
    if pd.notna(c_id):
        try:
            if int(c_id) not in valid_customer_ids:
                errors.append(f"Ghost Customer ID: {c_id}")
        except ValueError:
            errors.append(f"Malformed Customer ID: {c_id}")
    return errors

def validate_line_items(row):
    errors = []
    if row['line_item_amount'] < 0:
        errors.append("Negative Line Item Amount")
    if row['product_id'] > 1000:
        errors.append("Invalid Product ID")
    return errors

def ingest_and_validate(df, entity_name, date_cols=[], check_rules=None):
    rejected_rows = []
    clean_indices = []
    for col in date_cols:
        df[col] = pd.to_datetime(df[col], errors='coerce')

    for idx, row in df.iterrows():
        reasons = []
        for col in date_cols:
            if pd.isna(row[col]): reasons.append(f"Invalid Date in {col}")

        if check_rules:
            custom_errors = check_rules(row)
            if custom_errors: reasons.extend(custom_errors)

        if reasons:
            rejected_rows.append({
                'entity_name': entity_name,
                'transaction_id': str(row.get('transaction_id', row.get('line_item_id', 'N/A'))),
                'rejection_reason': "; ".join(reasons),
                'raw_data': json.dumps(row.astype(str).to_dict())
            })
        else:
            clean_indices.append(idx)

    df_clean = df.loc[clean_indices].copy()
    if rejected_rows:
        pd.DataFrame(rejected_rows).to_sql('rejected_records', conn, if_exists='append', index=False)
        print(f"   âš ï¸ {entity_name}: Rejected {len(rejected_rows)} bad records.")
    return df_clean

# --- 5. RUN PIPELINE ---
print("Running Ingestion...")
# Headers
df_headers_clean = ingest_and_validate(
    pd.read_csv('store_sales_header.csv'),
    'store_sales_header',
    date_cols=['transaction_date'],
    check_rules=validate_sales_header
)
df_headers_clean.rename(columns={'Store_id': 'store_id', 'Customer_phone': 'customer_phone'}, inplace=True)
df_headers_clean.to_sql('store_sales_header', conn, if_exists='append', index=False)

# Line Items
df_lines_clean = ingest_and_validate(
    pd.read_csv('store_sales_line_items.csv'),
    'store_sales_line_items',
    check_rules=validate_line_items
)
df_lines_clean.rename(columns={'Quantity': 'quantity'}, inplace=True)
valid_txn_ids = set(pd.read_sql("SELECT transaction_id FROM store_sales_header", conn)['transaction_id'])
df_lines_final = df_lines_clean[df_lines_clean['transaction_id'].isin(valid_txn_ids)]
df_lines_final.to_sql('store_sales_line_items', conn, if_exists='append', index=False)

print("âœ… Transaction Data Pipeline Complete.")

import sqlite3
import pandas as pd

# --- RE-CONNECT TO DATABASE (Required because previous block closed it) ---
conn = sqlite3.connect('retail_hackathon.db')
cursor = conn.cursor()

# ==========================================
# PART 3: LOYALTY ENGINE & REPORTING (CORRECTED)
# ==========================================

print("\n--- 1. RUNNING LOYALTY CALCULATION ---")

# A. Pull Transaction-Level Data (Instead of Pre-Aggregated)
sql_transactions = """
SELECT
    h.customer_id,
    h.transaction_id,
    h.total_amount,
    h.transaction_date
FROM store_sales_header h
WHERE h.customer_id IS NOT NULL
"""
df_txns = pd.read_sql(sql_transactions, conn)
df_txns['transaction_date'] = pd.to_datetime(df_txns['transaction_date'])

# B. Apply Loyalty Rules (MATCHING GROUND TRUTH LOGIC)
def calculate_points_strict(row):
    amount = row['total_amount']
    date = row['transaction_date']

    # Rule 1: Standard (1pt per $1)
    pts_std = amount * 1.0

    # Rule 2: Big Spender (1.5pt per $1 + 50 bonus if > 100)
    pts_big = 0
    if amount > 100:
        pts_big = (amount * 1.5) + 50

    # Rule 3: Weekend (2.0pt per $1 + 10 bonus if Sat/Sun)
    pts_wknd = 0
    if date.weekday() >= 5: # 5=Sat, 6=Sun
        pts_wknd = (amount * 2.0) + 10

    # Return the best applicable rule (Max)
    return int(max(pts_std, pts_big, pts_wknd))

# Apply logic row-by-row
df_txns['earned_points'] = df_txns.apply(calculate_points_strict, axis=1)

# C. Aggregate to Customer Level
df_spend = df_txns.groupby('customer_id').agg({
    'total_amount': 'sum',
    'earned_points': 'sum',
    'transaction_date': 'max' # Last seen
}).reset_index()

df_spend.rename(columns={
    'total_amount': 'total_spend',
    'earned_points': 'new_points',
    'transaction_date': 'last_seen'
}, inplace=True)

# D. Define RFM Segments
threshold_high = df_spend['total_spend'].quantile(0.90)
current_date = pd.to_datetime('today')
df_spend['days_since_last'] = (current_date - df_spend['last_seen']).dt.days

def determine_segment(row):
    if row['total_spend'] >= threshold_high:
        return 'High-Spender'
    elif row['days_since_last'] > 30:
        return 'At-Risk'
    else:
        return 'Standard'

df_spend['new_segment'] = df_spend.apply(determine_segment, axis=1)

# E. Update Database (Simulated Bulk Update)
print(f"Updating {len(df_spend)} customer records...")
for index, row in df_spend.iterrows():
    cursor.execute("""
        UPDATE customer_details
        SET total_loyalty_points = total_loyalty_points + ?,
            segment_id = ?
        WHERE customer_id = ?
    """, (row['new_points'], row['new_segment'], row['customer_id']))
conn.commit()
print("âœ… Loyalty Points Accrued and Segments Updated (Method: Transaction-Level Granularity).")

# --- CLOSE CONNECTION (Now we are done) ---
conn.close()